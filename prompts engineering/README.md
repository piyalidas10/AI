# ðŸ“œ Prompt Engineering

> **Prompt engineering is the practice of controlling LLM output by combining user questions, system instructions, and retrieved external data.**

When you talk to an AI model, there are two main things involved.
  -  First, there is the user prompt â€” this is the question you ask.
  -  Second, there are instructions, also called the system prompt.

Both the question and the instructions are sent to the LLM, which could be ChatGPT or DeepSeek.

The LLM then generates the answer according to the instructions, not just the question alone.

**Now, in more advanced setups, the LLM can also use RAG.**

  -  RAG stands for Retrieval Augmented Generation.
  -  With RAG, the LLM retrieves information from a database before generating the answer.
  -  So instead of answering only from its training, it uses external data to produce better, more accurate responses.

**Building this kind of system requires engineering and AI skills. Thatâ€™s what prompt engineering is about.**

Itâ€™s about controlling how the model answers, by giving clear instructions, not just asking a question.

> Prompt engineering is not about better questions â€” itâ€™s about better instructions.

### ðŸ§  What this Short is actually teaching

**Prompt Engineering =**
  -  Writing questions
  -  Writing instructions
  -  Controlling how the LLM answers

**System Prompt > User Prompt**
  -  Instructions decide tone, format, accuracy

**RAG adds knowledge**
  -  LLM + Database
  -  Real, up-to-date answers
  -  Enterprise-grade AI systems


