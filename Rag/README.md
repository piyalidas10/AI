# âœ… RAG (Retrieval-Augmented Generation)

**â€œRAG retrieves relevant knowledge from a vector database and injects it into the LLM prompt so the model generates grounded, context-aware answers instead of hallucinating.â€**

**ğŸ”·LLM does NOT read your documents directly. It retrieves relevant chunks from a vector DB and augments the prompt with them before generating an answer.**

### ğŸ§± RAG has TWO clear phases
```
1ï¸âƒ£ Indexing Phase (Offline / One-time)
2ï¸âƒ£ Query Phase (Runtime / Every question)
```

### ğŸ“Š Proper RAG Diagram (Clean & Correct)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        DATA SOURCES            â”‚
 â”‚  â€¢ PDFs  â€¢ Docs  â€¢ CSVs        â”‚
 â”‚  â€¢ DB rows â€¢ APIs              â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Document Loader & Parser     â”‚
 â”‚ (PDF â†’ text, CSV â†’ rows, etc.) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        Chunking                â”‚
 â”‚  (300â€“1000 tokens per chunk)   â”‚
 â”‚  + overlap (e.g. 50 tokens)    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     Embedding Model            â”‚
 â”‚  "Text â†’ Vector (numbers)"     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        Vector Database         â”‚
 â”‚  (Pinecone / FAISS / Chroma)   â”‚
 â”‚  Stores:                       â”‚
 â”‚  â€¢ vector                      â”‚
 â”‚  â€¢ original text chunk         â”‚
 â”‚  â€¢ metadata                    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
##### ğŸ”¹ 1. Read the Document (Indexing Phase)
**What happens**  
  -  PDFs â†’ text
  -  CSV â†’ rows
  -  Docs â†’ paragraphs

**Why**
  -  LLM cannot read raw files
  -  Everything must become plain text

##### ğŸ”¹ 2. Create Chunks
**Why chunking is mandatory**  
  -  LLM context window is limited
  -  Retrieval works better on smaller text units

**Typical values**
```
Chunk size: 300â€“1000 tokens
Overlap:    50â€“200 tokens
```
**Example**
```
Chunk 1: "RAG is a technique that..."
Chunk 2: "The indexing phase converts..."
```

##### ğŸ”¹ 3. Contextual Embedding
**Key concept**
```
Embedding = semantic meaning â†’ numbers
```
Example:
```
"This is an invoice" â†’ [0.021, -0.33, 0.89, ...]
```
  -  âœ” Same meaning â†’ vectors close together
  -  âœ” Different meaning â†’ vectors far apart

âš ï¸ Your slideâ€™s ##### is showing vectors

##### ğŸ”¹ 4. Store in Vector DB
Each record contains:
```
{
  vector: [0.021, -0.33, ...],
  text: "Original chunk text",
  metadata: {
    source: "invoice.pdf",
    page: 12
  }
}
```
**Popular Vector DBs**  
  -  Pinecone
  -  Chroma
  -  FAISS
  -  Weaviate

### ğŸ” What EXACTLY Happens When User Enters a Prompt (Step-by-Step)
â€œWhen a user enters a prompt, RAG converts it into an embedding, retrieves semantically similar document chunks from a vector database, injects them into the prompt, and then lets the LLM generate a grounded response.â€
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• QUERY TIME â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚         User Question          â”‚
 â”‚   "How does RAG work?"         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Question Embedding           â”‚
 â”‚  Same embedding model used     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Similarity Search (Top-K)      â”‚
 â”‚ Cosine / Dot / Euclidean       â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Retrieved Relevant Chunks     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Prompt Augmentation            â”‚
 â”‚ System Prompt + Context + Q    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚           LLM                  â”‚
 â”‚  GPT / LLaMA / Mistral         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        âœ… Grounded Answer
```
**Letâ€™s assume the user types: â€œHow does RAG work?â€**

> User Prompt is NOT sent directly to LLM
> ğŸš« Important misconception : The LLM does not answer immediately.

**Why?**
  -  LLM has no knowledge of your private documents.
  -  It must retrieve context first

**ğŸ”¹ 1. User Query (Prompt) â†’ Embedding**  
The question is converted into a vector:
```
User: "How does RAG work?"
â†’ [0.12, -0.44, 0.88, ...]
```

> Converted to vector using SAME embedding model
> âš ï¸ Same embedding model must be used (e.g., text-embedding-3-large, nomic-embed-text, etc.)
> âš ï¸ This is critical for correct similarity search

**ğŸ”¹ 6. Similarity Search**  
Vector DB compares:
```
Query Vector
   â†• cosine similarity
Stored Chunk Vectors
```

Returns:
```
Top-K most relevant chunks (K = 3â€“10)
Top 3â€“5 most relevant chunks
```

Example:
  -  Chunk from â€œRAG architecture.pdfâ€
  -  Chunk from â€œLLM retrieval notes.txtâ€

**ğŸ”¹ 7. Prompt Augmentation**  
Now the system builds the final prompt. LLM prompt becomes:
```
SYSTEM:
You are a helpful assistant.

CONTEXT:
Chunk 1: RAG has two phases...
Chunk 2: Embeddings represent semantic meaning...
Chunk 3: Vector databases enable similarity search...

USER QUESTION:
How does RAG work?
```

> ğŸ‘‰ This is why itâ€™s called Retrieval-Augmented Generation (RAG)

**ğŸ”¹ 8. LLM Generates Answer**  
Now the LLM:
  -  Reads only the provided context
  -  Generates a grounded, accurate answer using your private data
  -  Avoids hallucination

> âœ… Output is based on your documents, not training data

### ğŸ¯ Why RAG Is Powerful
| Problem            | Without RAG | With RAG |
| ------------------ | ----------- | -------- |
| Hallucination      | High        | Low      |
| Private data       | âŒ         | âœ…       |
| Up-to-date info    | âŒ         | âœ…       |
| Fine-tuning needed | Yes         | No       |
| Cost               | High        | Lower    |


