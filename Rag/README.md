# âœ… RAG (Retrieval-Augmented Generation)

<img src="https://github.com/piyalidas10/AI/blob/3f48d0a94bd2fb9b9b4227611974dbdf2ac676e0/Rag/img/RAG.png" width="600px" />

```
Data Sources  â†’  Vector DB  â†’  LLM
     (You own)      (RAG)       (Reasoning)
```

**â€œRAG retrieves relevant knowledge from a vector database and injects it into the LLM prompt so the model generates grounded, context-aware answers instead of hallucinating.â€**

**ğŸ”·LLM does NOT read your documents directly. It retrieves relevant chunks from a vector DB and augments the prompt with them before generating an answer.**

### ğŸ§± RAG has TWO clear phases
```
1ï¸âƒ£ Indexing Phase (Offline / One-time)
2ï¸âƒ£ Query Phase (Runtime / Every question)
```

<details>

<summary><strong>What is RAG ?</strong></summary>

## What is RAG ?
> chatGPT or Deepseek are good for generaic answers but if you ask a specific question related to your datatbase, it will fail. Because it has not seen that data. So along with the question we also provide the additional data and instructions how to access the data. Example "what are the table names and column names are present in the database?" These instructions must be very precise that is why data scientists and prompt engineers come into pictures. Which is why it is known as prompt engineering. When now you are asking a question you are supplimenting it or augmenting it with your database and additional instructions to formulate these answers. So the LLM goes and generate a response by retrieving the data from your database. The whole setup is known as Retrieval-Augmented Generation (RAG)

#### 1ï¸âƒ£ Why ChatGPT / DeepSeek fail for database-specific questions

  -  Models like ChatGPT or DeepSeek are trained on public, generic data
  -  They do not have access to:
       -  Your internal databases
       -  Company sales data
       -  Private tables or schemas

ğŸ“Œ Example: â€œWhat were my sales last quarter?â€

âŒ The LLM cannot answer this on its own because:
     It has never seen your database
     It cannot directly query your systems


</details>

<details>

<summary><strong>Proper RAG Diagram (Clean & Correct)</strong></summary>

## ğŸ“Š Proper RAG Diagram (Clean & Correct)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        DATA SOURCES            â”‚
 â”‚  â€¢ PDFs  â€¢ Docs  â€¢ CSVs        â”‚
 â”‚  â€¢ DB rows â€¢ APIs              â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Document Loader & Parser     â”‚
 â”‚ (PDF â†’ text, CSV â†’ rows, etc.) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        Chunking                â”‚
 â”‚  (300â€“1000 tokens per chunk)   â”‚
 â”‚  + overlap (e.g. 50 tokens)    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     Embedding Model            â”‚
 â”‚  "Text â†’ Vector (numbers)"     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        Vector Database         â”‚
 â”‚  (Pinecone / FAISS / Chroma)   â”‚
 â”‚  Stores:                       â”‚
 â”‚  â€¢ vector                      â”‚
 â”‚  â€¢ original text chunk         â”‚
 â”‚  â€¢ metadata                    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
##### ğŸ”¹ 1. Read the Document (Indexing Phase)
**What happens**  
  -  PDFs â†’ text
  -  CSV â†’ rows
  -  Docs â†’ paragraphs

**Why**
  -  LLM cannot read raw files
  -  Everything must become plain text

##### ğŸ”¹ 2. Create Chunks
**Why chunking is mandatory**  
  -  LLM context window is limited
  -  Retrieval works better on smaller text units

**Typical values**
```
Chunk size: 300â€“1000 tokens
Overlap:    50â€“200 tokens
```
**Example**
```
Chunk 1: "RAG is a technique that..."
Chunk 2: "The indexing phase converts..."
```

##### ğŸ”¹ 3. Contextual Embedding
**Key concept**
```
Embedding = semantic meaning â†’ numbers
```
Example:
```
"This is an invoice" â†’ [0.021, -0.33, 0.89, ...]
```
  -  âœ” Same meaning â†’ vectors close together
  -  âœ” Different meaning â†’ vectors far apart

âš ï¸ Your slideâ€™s ##### is showing vectors

##### ğŸ”¹ 4. Store in Vector DB
Each record contains:
```
{
  vector: [0.021, -0.33, ...],
  text: "Original chunk text",
  metadata: {
    source: "invoice.pdf",
    page: 12
  }
}
```
**Popular Vector DBs**  
  -  Pinecone
  -  Chroma
  -  FAISS
  -  Weaviate

</details>

<details>

<summary><strong>What EXACTLY Happens When User Enters a Prompt</strong></summary>

## ğŸ” What EXACTLY Happens When User Enters a Prompt (Step-by-Step)
â€œWhen a user enters a prompt, RAG converts it into an embedding, retrieves semantically similar document chunks from a vector database, injects them into the prompt, and then lets the LLM generate a grounded response.â€
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• QUERY TIME â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚         User Question          â”‚
 â”‚   "How does RAG work?"         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Question Embedding           â”‚
 â”‚  Same embedding model used     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Similarity Search (Top-K)      â”‚
 â”‚ Cosine / Dot / Euclidean       â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Retrieved Relevant Chunks     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Prompt Augmentation            â”‚
 â”‚ System Prompt + Context + Q    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚           LLM                  â”‚
 â”‚  GPT / LLaMA / Mistral         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        âœ… Grounded Answer
```
**Letâ€™s assume the user types: â€œHow does RAG work?â€**

> User Prompt is NOT sent directly to LLM
> ğŸš« Important misconception : The LLM does not answer immediately.

**Why?**
  -  LLM has no knowledge of your private documents.
  -  It must retrieve context first

**ğŸ”¹ 1. User Query (Prompt) â†’ Embedding**  
The question is converted into a vector:
```
User: "How does RAG work?"
â†’ [0.12, -0.44, 0.88, ...]
```

> Converted to vector using SAME embedding model
> âš ï¸ Same embedding model must be used (e.g., text-embedding-3-large, nomic-embed-text, etc.)
> âš ï¸ This is critical for correct similarity search

**ğŸ”¹ 2. Similarity Search**  
Vector DB compares:
```
Query Vector
   â†• cosine similarity
Stored Chunk Vectors
```

Returns:
```
Top-K most relevant chunks (K = 3â€“10)
Top 3â€“5 most relevant chunks
```

Example:
  -  Chunk from â€œRAG architecture.pdfâ€
  -  Chunk from â€œLLM retrieval notes.txtâ€

**ğŸ”¹ 3. Prompt Augmentation**  
Now the system builds the final prompt. LLM prompt becomes:
```
SYSTEM:
You are a helpful assistant.

CONTEXT:
Chunk 1: RAG has two phases...
Chunk 2: Embeddings represent semantic meaning...
Chunk 3: Vector databases enable similarity search...

USER QUESTION:
How does RAG work?
```

> ğŸ‘‰ This is why itâ€™s called Retrieval-Augmented Generation (RAG)

**ğŸ”¹ 4. LLM Generates Answer**  
Now the LLM:
  -  Reads only the provided context
  -  Generates a grounded, accurate answer using your private data
  -  Avoids hallucination

> âœ… Output is based on your documents, not training data

</details>

<details>

<summary><strong>RAG Questions & Answers</strong></summary>

### ğŸ¯ Why RAG Is Powerful
| Problem            | Without RAG | With RAG |
| ------------------ | ----------- | -------- |
| Hallucination      | High        | Low      |
| Private data       | âŒ         | âœ…       |
| Up-to-date info    | âŒ         | âœ…       |
| Fine-tuning needed | Yes         | No       |
| Cost               | High        | Lower    |

### 1ï¸âƒ£ Where does RAG get its data from?
ANs. RAG data sources are external systems. RAG does not invent data. You explicitly ingest data into the vector database during the indexing phase.

> â€œRAG gets its data from external knowledge sources such as documents, databases, or APIs that are ingested, embedded, and stored in a vector database. At query time, it retrieves relevant chunks from this indexed data to augment the LLMâ€™s response.â€

Common real-world data sources ğŸ‘‡
##### ğŸ“ Files & Documents (most common)
 - PDFs (manuals, policies, invoices)
 - Word / Text files
 - PowerPoint decks
 - CSV / Excel files

**âœ… Example:**
 - Company HR policy PDFs
 - Product documentation

##### ğŸ—„ï¸ Databases
 - PostgreSQL / MySQL
 - MongoDB
 - Data warehouse tables

**âœ… Example:**
 - Orders table
 - Customer support tickets
ğŸ‘‰ Rows â†’ text â†’ chunks â†’ embeddings

##### ğŸŒ APIs & Services
 - Internal microservices
 - REST / GraphQL APIs
 - SaaS tools (Jira, Confluence, Notion)

**âœ… Example:**
 - Jira issues
 - Confluence wiki pages

##### â˜ï¸ Cloud Storage
 - AWS S3
 - Azure Blob
 - Google Cloud Storage

**âœ… Example:**
 - Logs
 - Uploaded customer documents

##### ğŸ“¡ Streaming / Event Data (advanced)
 - Kafka topics
 - Event logs
 - IoT feeds (snapshotted)

âš ï¸ Usually summarized before embedding

### 2ï¸âƒ£ How does data reach RAG? (Important)
Ans. RAG never pulls data live at answer time (usually).

**âœ… Correct flow**
```
External Data Source
      â†“
Ingestion Pipeline
      â†“
Parsing + Cleaning
      â†“
Chunking
      â†“
Embedding
      â†“
Vector Database
```
ğŸ‘‰ At query time, RAG only talks to the vector DB, not the raw source.

### 3ï¸âƒ£ Example: Enterprise RAG Data Sources
**ğŸ¢ Company Chatbot**
| Source     | Purpose          |
| ---------- | ---------------- |
| PDFs       | HR policies      |
| DB tables  | Employee info    |
| Confluence | Engineering docs |
| Jira       | Incident history |

**ğŸ§ª Local Ollama RAG**
| Source       | Purpose        |
| ------------ | -------------- |
| Local folder | Markdown docs  |
| CSV files    | Knowledge base |
| SQLite       | App data       |

### 4ï¸âƒ£ What RAG does NOT use as a data source âŒ

âŒ LLM training data  
âŒ Internet (unless you build a crawler)  
âŒ User prompt history (unless you store it)  

LLM = reasoning engine, not knowledge store.

### 5ï¸âƒ£ Who decides the data source?

ğŸ‘‰ You do. RAG is just a pattern, not a product.

You decide:
 - What data to ingest
 - How often to update it
 - How fresh it should be

</details>


