# What is LangChain?

LangChain is a framework for building applications using LLMs. It acts as a middle layer between User/Application and the LLM. It simplifies development of advanced LLM-based applications.    
LangChain is used to build scalable, flexible, and advanced LLM applications with easy model switching and feature integration.

## Basic LLM Setup (Without LangChain)

<img src="imgs/Langchain.png" width="600px">

```
User ‚Üí Prompt ‚Üí LLM (Ollama / OpenAI / Google AI) ‚Üí Response
```

    -   Direct communication with LLM
    -   Works fine for simple prompt-response systems
    -   Not scalable for advanced features

## Setup With LangChain
```
User ‚Üí LangChain ‚Üí LLM ‚Üí LangChain ‚Üí User
```
LangChain sits in the middle

It can:
    -   Modify prompt
    -   Process output
    -   Add additional logic
    -   Connect external tools

## ‚≠ê Why We Use LangChain (Main Reasons)
**‚úÖ 1. Easy Model Switching (Very Important)**

LangChain makes it easy to switch models with minimal code change.

Example:
    -   From OpenAI GPT ‚Üí Local Ollama (LLaMA 3.1)
    -   From OpenAI ‚Üí Google Generative AI
    -   From Cloud model ‚Üí Local model

üëâ Usually requires just one line change    
üëâ No need to rewrite full application  

This makes applications flexible and scalable.

**‚úÖ 2. Easy Integration of Advanced Features**

LangChain simplifies adding:

üîπ Chat Memory
    -   Maintains conversation history
    -   Enables chatbot behavior

üîπ Knowledge Integration (RAG)
    -   Retrieval-Augmented Generation
    -   Connects LLM to:
        -   PDFs
        -   Databases
        -   Company documents
        -   Vector databases

üîπ Agents
    -   LLM can call tools
    -   Tools return results
    -   LLM decides next action

Example:
    -   Call weather API
    -   Query database
    -   Run calculator
    -   Execute search

üîπ API Creation
    -   Easy integration with FastAPI / backend systems


**üéØ Two Advanced Features Covered**

The instructor focuses on:
1. RAG (Retrieval-Augmented Generation)
    -   Adds external knowledge
    -   Solves LLM knowledge limitation
2. Agents
    -   LLM uses tools
    -   Tool output influences final response

These are the most common real-world LLM applications.

**üõ† Install required packages:**
```
pip install langchain
pip install langchain-core
pip install langchain-ollama
pip install langchain-community
```

After installing, you can start building:
    -   Simple prompt apps
    -   RAG systems
    -   Agent-based systems

### Knowledge Integration (RAG)

Now we want to do knowledge integration where we will give text files, PDF files or CSV files, or we may want to put all the relevant files in a folder. And then based on our input, we want to send a query and fetch only relevant documents or relevant part of the documents from this knowledge and give it back as part of prompt to the LLM. So instead of sending the whole PDF file or the entire text
or the entire data, we want to identify only relevant parts from this knowledge and send only that relevant part to the LLM so that it can answer the user query.

<img src="imgs/Knowledge_integration.png" width="600px">

**Why are we not sending the entire PDF file?**

There are two reasons for that.

    -   One is most models have a context window. That is you can only send certain amount of tokens in one prompt. So if you have a big PDF file, that will not be allowed in the prompt. You will not be able to send the entire PDF file.
    -   Second reason is that if you send a lot of information in the prompt, it becomes difficult for the LLM model to pick out relevant parts from it. It might give importance to non-important parts of the document, and you may not get the desired response from LLM. 

For this reason, in a rag system, we first fetch the relevant parts from our knowledge source and send that relevant part only to the LLM.

## üöÄ Why Do We Need LangSmith When We Already Have LangChain?

https://docs.langchain.com/langsmith/home

üîπ First Understand the Difference
    -   LangChain ‚Üí Framework to build LLM applications
    -   LangSmith ‚Üí Platform to debug, monitor, and evaluate LLM applications

üëâ Simple analogy:
    -   LangChain = Builder
    -   LangSmith = Inspector + Monitor + Evaluator

1Ô∏è‚É£ Debugging Complex Chains
-----------------------------------------------------------
When you build:
```
User ‚Üí PromptTemplate ‚Üí LLM ‚Üí Retriever ‚Üí Tool ‚Üí Final Output
```

If output is wrong:
    -   Is prompt wrong?
    -   Retriever failed?
    -   LLM hallucinated?
    -   Tool returned bad data?

üëâ LangChain alone does NOT give deep tracing.

‚úÖ LangSmith provides:
    -   Step-by-step execution trace
    -   Input/output at every node
    -   Token usage
    -   Latency tracking

2Ô∏è‚É£ Observability (Production Monitoring)
-----------------------------------------------------------
In production (e.g., your FastAPI API):

You need:
    -   How many users?
    -   Token consumption?
    -   Response time?
    -   Failure rate?
    -   Which prompts are underperforming?

LangChain ‚ùå does not provide monitoring dashboard  
LangSmith ‚úÖ provides production observability dashboard    

3Ô∏è‚É£ Evaluation & Testing (Very Important for RAG)
-----------------------------------------------------------
You‚Äôre working with RAG concepts. Now imagine:

You changed:
    -   Chunk size
    -   Embedding model
    -   Retriever strategy
    -   Prompt template

How do you know which version is better?

LangSmith allows:
    -   Dataset-based evaluation
    -   Run experiments
    -   Compare chain versions
    -   Score outputs (accuracy, relevance, faithfulness)

Without LangSmith ‚Üí Manual testing  
With LangSmith ‚Üí Scientific evaluation  

4Ô∏è‚É£ Prompt Versioning
-----------------------------------------------------------
In real projects:
    -   Prompt v1
    -   Prompt v2
    -   Prompt with system message
    -   Few-shot prompt

LangSmith helps:
    -   Track prompt versions
    -   Compare outputs
    -   Rollback bad versions
    -   LangChain alone cannot manage prompt experiments.

5Ô∏è‚É£ Team Collaboration (Enterprise Need)
-----------------------------------------------------------
If 5 developers work on same LLM system:

You need:
    -   Shared logs
    -   Shared traces
    -   Dataset management
    -   Experiment history

LangSmith supports team-level collaboration.

| Feature              | LangChain | LangSmith    |
| -------------------- | --------- | ------------ |
| Build chains         | ‚úÖ        | ‚ùå          |
| Prompt templates     | ‚úÖ        | ‚ùå          |
| RAG pipelines        | ‚úÖ        | ‚ùå          |
| Execution tracing    | Limited   | ‚úÖ Advanced  |
| Monitoring dashboard | ‚ùå        | ‚úÖ          |
| Evaluation framework | ‚ùå        | ‚úÖ          |
| Production analytics | ‚ùå        | ‚úÖ          |

#### üî• Real-World Example (Your FastAPI Setup)

Your flow:
```
Postman ‚Üí FastAPI ‚Üí LangChain ‚Üí Ollama
```

Without LangSmith:
    -   If response wrong ‚Üí you print() debug
    -   Hard to track token usage
    -   No experiment comparison

With LangSmith:
    -   Full trace of each request
    -   Token cost per request
    -   Prompt comparison
    -   Performance monitoring

You don‚Äôt need it if:
    -   Just learning LangChain
    -   Small personal project
    -   No production deployment
    -   No evaluation requirement

You MUST Use LangSmith
    -   Production LLM app
    -   RAG system
    -   Enterprise AI system
    -   Multi-user API
    -   Cost monitoring needed
    -   A/B prompt testing