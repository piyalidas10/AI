# 1ï¸âƒ£ FastAPI

### ğŸ”¹ What is FastAPI?
    -   A modern Python web framework
    -   Used to build REST APIs
    -   Very fast (built on Starlette + Pydantic)
    -   Automatic Swagger documentation

### ğŸ”¹ Why we use FastAPI in LLM projects?
    -   To create an API endpoint
    -   To expose LLM functionality over HTTP
    -   To connect frontend (Angular/React) with backend AI logic

### ğŸ”¹ Basic Example
```
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Hello World"}
```

If you run:
```
uvicorn main:app --reload
```

You get:
```
http://localhost:8000
```

Swagger docs:
```
http://localhost:8000/docs
```

### ğŸ”¹ In LLM Architecture

FastAPI acts as:
```
User â†’ FastAPI â†’ LangChain â†’ Ollama â†’ Response â†’ FastAPI â†’ User
```

It is the API layer.

# 2ï¸âƒ£ Postman
### ğŸ”¹ What is Postman?
    -   API testing tool
    -   Used to send HTTP requests
    -   Used to test backend endpoints

### ğŸ”¹ Why use Postman?
    -   To test your FastAPI endpoints
    -   To send POST requests with prompts
    -   To debug JSON request/response

### ğŸ”¹ Example

If your FastAPI has:
```
@app.post("/ask")
def ask_question(prompt: str):
    return {"response": prompt}
```

In Postman:
    -   Method â†’ POST
    -   URL â†’ http://localhost:8000/ask
    -   Body â†’ raw â†’ JSON
```
{
  "prompt": "Tell me about Kolkata cuisine"
}
```

### ğŸ”¹ Role in Architecture

Postman is: Testing Client

    -   It simulates:
    -   Frontend
    -   Mobile app
    -   Browser
    -   Any external client


# 3ï¸âƒ£ LangChain
### ğŸ”¹ What is LangChain?

    -   Python framework for building LLM applications
    -   Helps connect LLM with:
        -   Prompts
        -   Memory
        -   Tools
        -   RAG
        -   Vector DB

### ğŸ”¹ Why use LangChain?

Instead of directly calling LLM:
```
response = ollama.generate("Tell me about AI")
```

LangChain gives:
    -   Prompt templates
    -   Chains
    -   Agents
    -   RAG pipelines
    -   Structured output

### ğŸ”¹ Example with Ollama
```
from langchain_community.llms import Ollama

llm = Ollama(model="llama2")

response = llm.invoke("Tell me about Kolkata cuisine")
print(response)
```

### ğŸ”¹ Important Components
| Component      | Purpose              |
| -------------- | -------------------- |
| PromptTemplate | Format prompt        |
| LLM            | Connect to model     |
| Chain          | Combine prompt + LLM |
| Memory         | Conversation history |
| Retriever      | Used in RAG          |
| Agent          | Tool-calling logic   |

### ğŸ”¹ In Architecture

LangChain is: AI Orchestration Layer

It manages:
    -   Prompt engineering
    -   RAG
    -   Tool usage
    -   Memory

# 4ï¸âƒ£ Ollama
### ğŸ”¹ What is Ollama?
    -   Tool to run LLMs locally
    -   Runs models like:
        -   Llama2
        -   Mistral
        -   Gemma
        -   DeepSeek

### ğŸ”¹ Why Ollama?
    -   No API cost
    -   Runs locally
    -   No internet required
    -   Data privacy

### ğŸ”¹ Install (Windows)
```
ollama pull llama2
ollama run llama2
```

### ğŸ”¹ How it works internally
```
Prompt â†’ Model â†’ Tokenization â†’ Transformer â†’ Output tokens â†’ Text
```

### ğŸ”¹ Ollama API (Local)

Runs at:
```
http://localhost:11434
```

You can call it directly via REST API OR via LangChain.

